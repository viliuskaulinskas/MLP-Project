{"cells":[{"metadata":{"_uuid":"e23fb30d-fed0-491c-b87f-b2f73997e768","_cell_guid":"a5ac26cc-df85-44d0-b516-28f48120a075","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install dependencies: \n!pip install pyyaml==5.1\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n!gcc --version\nimport torch\nassert torch.__version__.startswith(\"1.7\")\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting pyyaml==5.1\n  Downloading PyYAML-5.1.tar.gz (274 kB)\n\u001b[K     |████████████████████████████████| 274 kB 224 kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyyaml\n  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44074 sha256=66799ee69cd329844bd4b8b00e5570e6c49a9af40604983f896e899745784355\n  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\nSuccessfully built pyyaml\nInstalling collected packages: pyyaml\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 5.3.1\n\u001b[31mERROR: Cannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n1.7.0 True\ngcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nLooking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html\nCollecting detectron2\n  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/detectron2-0.3%2Bcu102-cp37-cp37m-linux_x86_64.whl (6.8 MB)\n\u001b[K     |████████████████████████████████| 6.8 MB 1.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from detectron2) (0.8.7)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from detectron2) (1.6.0)\nCollecting fvcore>=0.1.2\n  Downloading fvcore-0.1.3.post20210204.tar.gz (35 kB)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from detectron2) (0.1.8)\nRequirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.7/site-packages (from detectron2) (7.2.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from detectron2) (2.4.1)\nRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.7/site-packages (from detectron2) (4.55.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from detectron2) (1.1.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from detectron2) (0.18.2)\nCollecting pydot\n  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\nCollecting pycocotools>=2.0.2\n  Downloading pycocotools-2.0.2.tar.gz (23 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from detectron2) (3.3.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore>=0.1.2->detectron2) (1.19.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore>=0.1.2->detectron2) (5.3.1)\nCollecting iopath>=0.1.2\n  Downloading iopath-0.1.3.tar.gz (10 kB)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath>=0.1.2->fvcore>=0.1.2->detectron2) (2.1.0)\nRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools>=2.0.2->detectron2) (49.6.0.post20201009)\nRequirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools>=2.0.2->detectron2) (0.29.21)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (1.3.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (0.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->detectron2) (1.15.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (0.36.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (0.4.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (3.3.3)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.32.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (0.10.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.8.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.0.1)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (3.14.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (2.25.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.24.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.1.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->detectron2) (3.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->detectron2) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->detectron2) (3.4.0)\nBuilding wheels for collected packages: fvcore, iopath, pycocotools\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.3.post20210204-py3-none-any.whl size=44943 sha256=efc04e5fddff64263842f660b8794c7ec28f3daa0a122981b2a2382d29b7f399\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/8c/124367ec901d4b48b5ba4c0226c0a8239815b4e969ad15cc7a\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.3-py3-none-any.whl size=11169 sha256=726c5fccf3d79ad22ea8eca91c00796e000706a398d1294954146c658f8ebbd2\n  Stored in directory: /root/.cache/pip/wheels/33/44/20/06445612ad8cf4ad6250aee85516e9499d5a49151ef5358164\n","name":"stdout"},{"output_type":"stream","text":"  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272627 sha256=acb4ff1bb225567622d6893a2fff9a99fa8adff16e2c3e57480b09eb2fbbd239\n  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\nSuccessfully built fvcore iopath pycocotools\nInstalling collected packages: iopath, pydot, pycocotools, fvcore, detectron2\nSuccessfully installed detectron2-0.3+cu102 fvcore-0.1.3.post20210204 iopath-0.1.3 pycocotools-2.0.2 pydot-1.4.1\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n\n# import some common libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import  StratifiedShuffleSplit\nimport os, json, cv2, random\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog \n\nimport copy\nimport logging\nimport numpy as np\nfrom typing import Callable, List, Union\nimport torch\n\nfrom detectron2.config import configurable\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import transforms as T\n\nfrom detectron2.data import detection_utils as utils\nimport copy\nimport detectron2.data.transforms as T\nimport matplotlib.pyplot as plt\nfrom detectron2.data import DatasetMapper\nimport torch\nimport os\nimport numpy as np\n\nfrom detectron2.config import configurable","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirpath = '../input/vinbigdata-competition-jpg-data-3x-downsampled'\n\ndf = pd.read_csv(f'{dirpath}/train_downsampled.csv')\ndf.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(67914, 8)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                           image_id          class_name  class_id rad_id  \\\n0  50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n1  21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n2  9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n3  051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n4  063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n\n   x_min  y_min  x_max  y_max  \n0    NaN    NaN    NaN    NaN  \n1    NaN    NaN    NaN    NaN  \n2  230.0  458.0  551.0  610.0  \n3  421.0  247.0  537.0  339.0  \n4    NaN    NaN    NaN    NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>class_name</th>\n      <th>class_id</th>\n      <th>rad_id</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50a418190bc3fb1ef1633bf9678929b3</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21a10246a5ec7af151081d0cd6d65dc9</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9a5094b2563a1ef3ff50dc5c7ff71345</td>\n      <td>Cardiomegaly</td>\n      <td>3</td>\n      <td>R10</td>\n      <td>230.0</td>\n      <td>458.0</td>\n      <td>551.0</td>\n      <td>610.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>051132a778e61a86eb147c7c6f564dfe</td>\n      <td>Aortic enlargement</td>\n      <td>0</td>\n      <td>R10</td>\n      <td>421.0</td>\n      <td>247.0</td>\n      <td>537.0</td>\n      <td>339.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>063319de25ce7edb9b1c6b8881290140</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['w'], df['h'] = df['x_max'] - df['x_min'], df['y_max'] - df['y_min']\ndf['area'] = df['w'] * df['h']\ndf.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"                           image_id          class_name  class_id rad_id  \\\n0  50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n1  21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n2  9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n3  051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n4  063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n\n   x_min  y_min  x_max  y_max      w      h     area  \n0    NaN    NaN    NaN    NaN    NaN    NaN      NaN  \n1    NaN    NaN    NaN    NaN    NaN    NaN      NaN  \n2  230.0  458.0  551.0  610.0  321.0  152.0  48792.0  \n3  421.0  247.0  537.0  339.0  116.0   92.0  10672.0  \n4    NaN    NaN    NaN    NaN    NaN    NaN      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>class_name</th>\n      <th>class_id</th>\n      <th>rad_id</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n      <th>w</th>\n      <th>h</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50a418190bc3fb1ef1633bf9678929b3</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21a10246a5ec7af151081d0cd6d65dc9</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9a5094b2563a1ef3ff50dc5c7ff71345</td>\n      <td>Cardiomegaly</td>\n      <td>3</td>\n      <td>R10</td>\n      <td>230.0</td>\n      <td>458.0</td>\n      <td>551.0</td>\n      <td>610.0</td>\n      <td>321.0</td>\n      <td>152.0</td>\n      <td>48792.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>051132a778e61a86eb147c7c6f564dfe</td>\n      <td>Aortic enlargement</td>\n      <td>0</td>\n      <td>R10</td>\n      <td>421.0</td>\n      <td>247.0</td>\n      <td>537.0</td>\n      <td>339.0</td>\n      <td>116.0</td>\n      <td>92.0</td>\n      <td>10672.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>063319de25ce7edb9b1c6b8881290140</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nprint(df.shape)\ndf_dd = df.drop_duplicates('image_id')\nprint(df_dd.shape)\ndf_dd = df_dd.reset_index()\nsss.get_n_splits(df_dd['image_id'], df_dd['class_id'])\nfor train_index, test_index in sss.split(df_dd['image_id'], df_dd['class_id']):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = df_dd['image_id'][train_index], df_dd['image_id'][test_index]\n    y_train, y_test = df_dd['class_id'][train_index], df_dd['class_id'][test_index]","execution_count":7,"outputs":[{"output_type":"stream","text":"(67914, 11)\n(15000, 11)\nTRAIN: [10223 11258 11436 ... 12186  5843 12594] TEST: [12600  6685 12389 ... 13066  3822   223]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = df.drop_duplicates('class_id').sort_values('class_id')[['class_name']].values[:-1].ravel().tolist()\nprint(classes)\nthing_classes = {class_name: index for index, class_name in enumerate(classes)}\nprint(thing_classes)","execution_count":8,"outputs":[{"output_type":"stream","text":"['Aortic enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly', 'Consolidation', 'ILD', 'Infiltration', 'Lung Opacity', 'Nodule/Mass', 'Other lesion', 'Pleural effusion', 'Pleural thickening', 'Pneumothorax', 'Pulmonary fibrosis']\n{'Aortic enlargement': 0, 'Atelectasis': 1, 'Calcification': 2, 'Cardiomegaly': 3, 'Consolidation': 4, 'ILD': 5, 'Infiltration': 6, 'Lung Opacity': 7, 'Nodule/Mass': 8, 'Other lesion': 9, 'Pleural effusion': 10, 'Pleural thickening': 11, 'Pneumothorax': 12, 'Pulmonary fibrosis': 13}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Read in dicom files"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"12600    f87d3fef16e2bb2ef72179864c9f6cef\n6685     1a15da91ec46d4887b40c65866fba831\n12389    95badc82bced45380cf4275bb11d2bb9\n7013     2a4a459820226a8b7683df0df4adac6f\n2310     73b83ade1fb63fc7c966c1d776323ad0\n                       ...               \n2636     bde4bfa149bc13cee2de7c2e942979e7\n1808     fe358fa490f9b97a3be8142961dd69f6\n13066    97fd11ea81a44af39e0f8bac372acb75\n3822     0fdba5b4298773f488709251e49b26a2\n223      6a956ce4e56235dd8de081a965a36c2a\nName: image_id, Length: 3000, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### DATASET PREPARING: CONERTING CSV TO DICTIONARY WHICH CONTAINS ANNOTATION AND IMAGE PATH ETC.\n\nfrom detectron2.structures import BoxMode\nfrom pathlib import Path\n\ndef chest_dicts_train(images, img_dir = '../input/vinbigdata-competition-jpg-data-3x-downsampled/train/train', load_cache=True):\n    cache_path = Path(\".\") / f\"dataset_dicts_cache_train.pkl\"\n    if load_cache == False:\n        dataset_dicts = []\n        for idx, v in enumerate(images):\n            record = {}\n\n            filename = os.path.join(img_dir, v + '.jpg')\n\n            image = cv2.imread(filename)\n            height, width, ch = image.shape\n\n            record[\"file_name\"] = filename\n            record[\"image_id\"] = idx\n            record[\"height\"] = height # RANDOM Not Req\n            record[\"width\"] = width # RANDOM Not Req\n\n            annos = df[df.image_id == v]\n            objs = []\n            for _, anno in annos.iterrows():\n                if anno.class_id != 14:\n\n                    obj = {\n                        \"bbox\": [int(anno.x_min), int(anno.y_min), int(anno.w), int(anno.h)],\n                        \"bbox_mode\": BoxMode.XYWH_ABS,\n                        \"category_id\": int(anno.class_id)\n                    }\n                    objs.append(obj)\n            record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n            \n        with open(cache_path, mode=\"wb\") as f:\n                pickle.dump(dataset_dicts, f)\n    else:\n        with open(cache_path, mode=\"rb\") as f:\n            dataset_dicts = pickle.load(f)\n    return dataset_dicts\n\ndef chest_dicts_test(images, img_dir = '../input/vinbigdata-competition-jpg-data-3x-downsampled/train/train', load_cache=True):\n    cache_path = Path(\".\") / f\"dataset_dicts_cache_test.pkl\"\n    if load_cache == False:\n        dataset_dicts = []\n        for idx, v in enumerate(images):\n            record = {}\n\n            filename = os.path.join(img_dir, v + '.jpg')\n\n            image = cv2.imread(filename)\n            height, width, ch = image.shape\n\n            record[\"file_name\"] = filename\n            record[\"image_id\"] = idx + 12000\n            record[\"height\"] = height # RANDOM Not Req\n            record[\"width\"] = width # RANDOM Not Req\n\n            annos = df[df.image_id == v]\n            objs = []\n            for _, anno in annos.iterrows():\n                if anno.class_id != 14:\n\n                    obj = {\n                        \"bbox\": [int(anno.x_min), int(anno.y_min), int(anno.w), int(anno.h)],\n                        \"bbox_mode\": BoxMode.XYWH_ABS,\n                        \"category_id\": int(anno.class_id)\n                    }\n                    objs.append(obj)\n            record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n                pickle.dump(dataset_dicts, f)\n    else:\n        with open(cache_path, mode=\"rb\") as f:\n            dataset_dicts = pickle.load(f)\n    return dataset_dicts\ndef train():\n    return chest_dicts_train(X_train, load_cache=True)\ndef val():\n    return chest_dicts_test(X_test, load_cache=True)\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DatasetCatalog.register(\"chest_Train\", train)\nMetadataCatalog.get(\"chest_Train\").thing_classes=classes\nDatasetCatalog.register(\"chest_Val\",val)\nMetadataCatalog.get(\"chest_Val\").thing_classes=classes","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTo calculate & record validation loss\n\nOriginal code from https://medium.com/@apofeniaco/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\nby @apofeniaco\n\"\"\"\nimport numpy as np\nimport logging\n\nfrom detectron2.engine.hooks import HookBase\nfrom detectron2.utils.logger import log_every_n_seconds\nimport detectron2.utils.comm as comm\nimport torch\nimport time\nimport datetime\n\n\nclass LossEvalHook(HookBase):\n    def __init__(self, eval_period, model, data_loader):\n        self._model = model\n        self._period = eval_period\n        self._data_loader = data_loader\n\n    def _do_loss_eval(self):\n        # Copying inference_on_dataset from evaluator.py\n        total = len(self._data_loader)\n        num_warmup = min(5, total - 1)\n\n        start_time = time.perf_counter()\n        total_compute_time = 0\n        losses = []\n        for idx, inputs in enumerate(self._data_loader):\n            if idx == num_warmup:\n                start_time = time.perf_counter()\n                total_compute_time = 0\n            start_compute_time = time.perf_counter()\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            total_compute_time += time.perf_counter() - start_compute_time\n            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n            seconds_per_img = total_compute_time / iters_after_start\n            if idx >= num_warmup * 2 or seconds_per_img > 5:\n                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n                log_every_n_seconds(\n                    logging.INFO,\n                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n                        idx + 1, total, seconds_per_img, str(eta)\n                    ),\n                    n=5,\n                )\n            loss_batch = self._get_loss(inputs)\n            losses.append(loss_batch)\n        mean_loss = np.mean(losses)\n        # self.trainer.storage.put_scalar('validation_loss', mean_loss)\n        comm.synchronize()\n\n        # return losses\n        return mean_loss\n\n    def _get_loss(self, data):\n        # How loss is calculated on train_loop\n        metrics_dict = self._model(data)\n        metrics_dict = {\n            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n            for k, v in metrics_dict.items()\n        }\n        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n        return total_losses_reduced\n\n    def after_step(self):\n        next_iter = int(self.trainer.iter) + 1\n        is_final = next_iter == self.trainer.max_iter\n        if is_final or (self._period > 0 and next_iter % self._period == 0):\n            mean_loss = self._do_loss_eval()\n            self.trainer.storage.put_scalars(validation_loss=mean_loss)\n            print(\"validation do loss eval\", mean_loss)\n        else:\n            pass\n            # self.trainer.storage.put_scalars(timetest=11)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from detectron2.data import detection_utils as utils\nfrom detectron2.data import build_detection_train_loader\nfrom PIL import Image\n# writing a new DatasetMapper as we have dicom files\n\nclass DatasetMapper:\n    \n    \n\n    @configurable\n    def __init__(\n        self,\n        is_train: bool,\n        *,\n        augmentations: List[Union[T.Augmentation, T.Transform]],\n        image_format: str = 'BGR',\n        use_instance_mask: bool = False,\n        use_keypoint: bool = False,\n        instance_mask_format = \"polygon\",\n        keypoint_hflip_indices = None,\n        precomputed_proposal_topk= None,\n        recompute_boxes: bool = False,\n    ):\n        \"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            is_train: whether it's used in training or inference\n            augmentations: a list of augmentations or deterministic transforms to apply\n            image_format: an image format supported by :func:`detection_utils.read_image`.\n            use_instance_mask: whether to process instance segmentation annotations, if available\n            use_keypoint: whether to process keypoint annotations if available\n            instance_mask_format: one of \"polygon\" or \"bitmask\". Process instance segmentation\n                masks into this format.\n            keypoint_hflip_indices: see :func:`detection_utils.create_keypoint_hflip_indices`\n            precomputed_proposal_topk: if given, will load pre-computed\n                proposals from dataset_dict and keep the top k proposals for each image.\n            recompute_boxes: whether to overwrite bounding box annotations\n                by computing tight bounding boxes from instance mask annotations.\n        \"\"\"\n\n        # fmt: off\n        self.is_train               = is_train\n        self.augmentations          = T.AugmentationList(  augmentations)\n        self.image_format           = image_format\n        \n        # fmt: on\n        logger = logging.getLogger(__name__)\n        mode = \"training\" if is_train else \"inference\"\n        logger.info(f\"[DatasetMapper] Augmentations used in {mode}: {augmentations}\")\n\n    @classmethod\n    def from_config(cls, cfg, is_train: bool = True):\n        augs = utils.build_augmentation(cfg, is_train)\n        if cfg.INPUT.CROP.ENABLED and is_train:\n            augs.insert(0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))\n            recompute_boxes = cfg.MODEL.MASK_ON\n        else:\n            recompute_boxes = False\n\n\n        ret = {\n            \"is_train\": is_train,\n            \"augmentations\": augs,\n            \"image_format\": cfg.INPUT.FORMAT,\n            \"use_instance_mask\": cfg.MODEL.MASK_ON,\n            \"instance_mask_format\": cfg.INPUT.MASK_FORMAT,\n            \"use_keypoint\": cfg.MODEL.KEYPOINT_ON,\n            \"recompute_boxes\": recompute_boxes,\n        }\n\n        return ret\n\n    def __call__(self, dataset_dict):\n        # print(dataset_dict)\n        dataset_dict = copy.deepcopy(dataset_dict)\n        image = utils.read_image(dataset_dict[\"file_name\"])\n        \n        # I'm not sure about these augmentations, need to properly go over them\n        auginput = T.AugInput(image)\n        transform = self.augmentations(auginput)\n        image = np.expand_dims(auginput.image, axis=2).copy()\n        image = torch.from_numpy(image.transpose(2, 0, 1))\n        annos = [\n            utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n            for annotation in dataset_dict.pop(\"annotations\")\n        ]\n        return {\n        # create the format that the model expects\n        \"image\": image,\n        \"instances\": utils.annotations_to_instances(annos, image.shape[1:])\n        }","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from detectron2.engine import DefaultTrainer\nfrom detectron2.data import MetadataCatalog, build_detection_train_loader,build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer(DefaultTrainer):\n    \n    \n#     @classmethod\n#     def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n#         if output_folder is None:\n#             output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n#         # return COCOEvaluator(dataset_name, cfg, True, output_folder)\n#         return COCOEvaluator(dataset_name, tasks=(\"bbox\"), distributed=False, output_dir=output_folder)\n    \n#     @classmethod\n#     def build_test_loader(cls, cfg, dataset_name):\n#         return build_detection_test_loader(cfg,dataset_name,\n#                 mapper=DatasetMapper(cfg, is_train = False))\n    \n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        return build_detection_test_loader(cfg,dataset_name,\n                mapper=DatasetMapper(cfg, is_train = False))\n    \n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(dataset =train(),\n                mapper=DatasetMapper(cfg, is_train = True),\n                aspect_ratio_grouping=False, \n                total_batch_size = cfg.SOLVER.IMS_PER_BATCH)\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return COCOEvaluator(dataset_name, (\"bbox\",), False, output_dir=output_folder)\n\n    \n    def build_hooks(self):\n        hooks = super(Trainer, self).build_hooks()\n        cfg = self.cfg\n        if len(cfg.DATASETS.TEST) > 0:\n            loss_eval_hook = LossEvalHook(\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                Trainer.build_test_loader(cfg, cfg.DATASETS.TEST[0]),\n            )\n            hooks.insert(-1, loss_eval_hook)\n\n        return hooks\n    \n    \n    ","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Batch = 10\nEpochs = 1\nsteps = 120  #  ### INCREASE THE STEPS   (len(X_train) // Batch) * Epochs \ncfg = get_cfg()\nNAME = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\ncfg.merge_from_file(model_zoo.get_config_file(NAME))\ncfg.DATASETS.TRAIN = (\"chest_Train\",)\ncfg.DATASETS.TEST = (\"chest_Val\", )\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS =  model_zoo.get_checkpoint_url(NAME)\ncfg.SOLVER.IMS_PER_BATCH = Batch\ncfg.CUDNN_BENCHMARK =  True\ncfg.MODEL.RETINANET.NUM_CLASSES  = len(classes)\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\ncfg.SOLVER.MAX_ITER = steps  \ncfg.OUTPUT_DIR = './output'\ncfg.MODEL.PIXEL_MEAN = [103.530]\ncfg.MODEL.PIXEL_STD = [1.0]\ncfg.SOLVER.CHECKPOINT_PERIOD = 1000\ncfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 0.95\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()\n#     cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n#     print(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\n#     cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0  # set a custom testing threshold\n#     print(\"Changed  thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n#     predictor = DefaultPredictor(cfg)\n#     trainer.test(cfg, trainer.model)\n\n\n    \n    \n#    evaluator = COCOEvaluator(\"chest_Val\", (\"bbox\", ), False, output_dir=\"./output/\")\n#     val_loader = build_detection_test_loader(cfg, \"chest_Val\", mapper=DatasetMapper(cfg, is_train = False))\n#     print(inference_on_dataset(trainer.model, val_loader, evaluator))\n    \n    ","execution_count":89,"outputs":[{"output_type":"stream","text":"\u001b[32m[02/07 02:49:35 d2.engine.defaults]: \u001b[0mModel:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n    )\n  )\n)\n","name":"stdout"},{"output_type":"stream","text":"\u001b[32m[02/07 02:49:35 d2.data.common]: \u001b[0mSerializing 12000 elements to byte tensors and concatenating them all ...\n\u001b[32m[02/07 02:49:35 d2.data.common]: \u001b[0mSerialized dataset takes 3.16 MiB\n\u001b[32m[02/07 02:50:32 d2.data.common]: \u001b[0mSerializing 3000 elements to byte tensors and concatenating them all ...\n\u001b[32m[02/07 02:50:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.79 MiB\n\u001b[32m[02/07 02:50:33 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n\u001b[32m[02/07 02:50:57 d2.utils.events]: \u001b[0m eta: 0:24:26  iter: 19  total_loss: 0.597  loss_cls: 0.08146  loss_box_reg: 0.004429  loss_rpn_cls: 0.471  loss_rpn_loc: 0.04155  time: 1.2432  data_time: 0.1215  lr: 4.9922e-06  max_mem: 12171M\n\u001b[32m[02/07 02:51:22 d2.utils.events]: \u001b[0m eta: 0:23:59  iter: 39  total_loss: 0.563  loss_cls: 0.1247  loss_box_reg: 0.01069  loss_rpn_cls: 0.4283  loss_rpn_loc: 0.03273  time: 1.2443  data_time: 0.1313  lr: 9.9642e-06  max_mem: 12171M\n\u001b[32m[02/07 02:51:47 d2.utils.events]: \u001b[0m eta: 0:23:34  iter: 59  total_loss: 0.7893  loss_cls: 0.109  loss_box_reg: 0.007446  loss_rpn_cls: 0.5721  loss_rpn_loc: 0.04461  time: 1.2400  data_time: 0.1191  lr: 1.4896e-05  max_mem: 12171M\n\u001b[32m[02/07 02:52:11 d2.utils.events]: \u001b[0m eta: 0:23:06  iter: 79  total_loss: 0.4497  loss_cls: 0.09615  loss_box_reg: 0.007533  loss_rpn_cls: 0.3306  loss_rpn_loc: 0.0246  time: 1.2360  data_time: 0.1259  lr: 1.9767e-05  max_mem: 12171M\n\u001b[32m[02/07 02:52:36 d2.utils.events]: \u001b[0m eta: 0:22:41  iter: 99  total_loss: 0.4029  loss_cls: 0.07368  loss_box_reg: 0.009708  loss_rpn_cls: 0.2847  loss_rpn_loc: 0.02741  time: 1.2369  data_time: 0.1242  lr: 2.4558e-05  max_mem: 12171M\n\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[02/07 02:52:44 d2.engine.train_loop]: \u001b[0mException during training:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 134, in train\n    self.run_step()\n  File \"/opt/conda/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 423, in run_step\n    self._trainer.run_step()\n  File \"/opt/conda/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 236, in run_step\n    losses.backward()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/tensor.py\", line 221, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 132, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: CUDA out of memory. Tried to allocate 776.00 MiB (GPU 0; 15.90 GiB total capacity; 11.50 GiB already allocated; 435.75 MiB free; 14.58 GiB reserved in total by PyTorch)\n\u001b[32m[02/07 02:52:44 d2.engine.hooks]: \u001b[0mOverall training speed: 103 iterations in 0:02:08 (1.2490 s / it)\n\u001b[32m[02/07 02:52:44 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:08 (0:00:00 on hooks)\n\u001b[32m[02/07 02:52:44 d2.utils.events]: \u001b[0m eta: 0:22:32  iter: 105  total_loss: 0.3821  loss_cls: 0.07368  loss_box_reg: 0.007277  loss_rpn_cls: 0.2677  loss_rpn_loc: 0.02631  time: 1.2338  data_time: 0.1257  lr: 2.5741e-05  max_mem: 12508M\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 776.00 MiB (GPU 0; 15.90 GiB total capacity; 11.50 GiB already allocated; 435.75 MiB free; 14.58 GiB reserved in total by PyTorch)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-7a8a791a4a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             assert hasattr(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 776.00 MiB (GPU 0; 15.90 GiB total capacity; 11.50 GiB already allocated; 435.75 MiB free; 14.58 GiB reserved in total by PyTorch)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}