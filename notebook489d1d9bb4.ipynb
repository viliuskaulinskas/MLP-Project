{"cells":[{"metadata":{"_uuid":"e23fb30d-fed0-491c-b87f-b2f73997e768","_cell_guid":"a5ac26cc-df85-44d0-b516-28f48120a075","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install dependencies: \n!pip install pyyaml==5.1\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n!gcc --version\nimport torch\nassert torch.__version__.startswith(\"1.7\")\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n\n# import some common libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import  StratifiedShuffleSplit\nimport os, json, cv2, random\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog \n\nimport copy\nimport logging\nimport numpy as np\nfrom typing import Callable, List, Union\nimport torch\n\nfrom detectron2.config import configurable\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import transforms as T\n\nfrom detectron2.data import detection_utils as utils\nimport copy\nimport detectron2.data.transforms as T\nimport matplotlib.pyplot as plt\nfrom detectron2.data import DatasetMapper\nimport torch\nimport os\nimport numpy as np\n\nfrom detectron2.config import configurable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detectron2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 67914 entries of abnormalities/no abnormalities detected\n\ndirpath = '../input/vinbigdata-competition-jpg-data-3x-downsampled'\n\ndf = pd.read_csv(f'{dirpath}/train_downsampled.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimensions, area of identified abnormality plot\ndf['w'], df['h'] = df['x_max'] - df['x_min'], df['y_max'] - df['y_min']\ndf['area'] = df['w'] * df['h']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Images: ', len(os.listdir(f'{dirpath}/train'))) # 1 ???","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nprint(df.shape)\n\n# I don't think we should drop duplicates - Vilius\ndf_dd = df.drop_duplicates('image_id')\nprint(df_dd.shape)\n\ndf_dd = df_dd.reset_index()\nsss.get_n_splits(df_dd['image_id'], df_dd['class_id'])\n\n# ?\nfor train_index, test_index in sss.split(df_dd['image_id'], df_dd['class_id']):\n    print(\"TRAIN:\", train_index)\n    print(\"TEST:\", test_index)\n    X_train, X_test = df_dd['image_id'][train_index], df_dd['image_id'][test_index]\n    y_train, y_test = df_dd['class_id'][train_index], df_dd['class_id'][test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Class names (abnormalities) and indexes, excluding \"no abnormality\", in dictionary\n# classes = df.drop_duplicates('class_id').sort_values('class_id')[['class_name']].values[:-1].ravel().tolist()\n# print(classes)\n# thing_classes = {class_name: index for index, class_name in enumerate(classes)}\n# print(thing_classes)\n# Class names (abnormalities) and indexes, excluding \"no abnormality\", in dictionary\nclasses = df.drop_duplicates('class_id').sort_values('class_id')[['class_name']].values[:-1].ravel().tolist()\nprint(classes)\nthing_classes = {class_name: index for index, class_name in enumerate(classes)}\nprint(thing_classes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read in dicom files"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"X_train.head()"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### DATASET PREPARING: CONERTING CSV TO DICTIONARY WHICH CONTAINS ANNOTATION AND IMAGE PATH ETC.\n\nfrom detectron2.structures import BoxMode\nfrom pathlib import Path\n\ndef chest_dicts_train(images, img_dir = '../input/vinbigdata-competition-jpg-data-3x-downsampled/train/train', load_cache=True):\n#     cache_path = Path(\".\") / f\"dataset_dicts_cache_train.pkl\"\n#     if load_cache == False:\n    dataset_dicts = []\n    for idx, v in enumerate(images):\n        record = {}\n\n        # Read the images\n        filename = os.path.join(img_dir, v + '.jpg')\n\n        image = cv2.imread(filename)\n        height, width, ch = image.shape\n\n        record[\"file_name\"] = filename\n        record[\"image_id\"] = idx\n        record[\"height\"] = height # RANDOM Not Req\n        record[\"width\"] = width # RANDOM Not Req\n\n        annos = df[df.image_id == v]\n        objs = []\n        for _, anno in annos.iterrows():\n            if anno.class_id != 14:\n\n                obj = {\n                    \"bbox\": [int(anno.x_min), int(anno.y_min), int(anno.w), int(anno.h)],\n                    \"bbox_mode\": BoxMode.XYWH_ABS,\n                    \"category_id\": int(anno.class_id)\n                }\n                objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n            \n#         with open(cache_path, mode=\"wb\") as f:\n#                 pickle.dump(dataset_dicts, f)\n#     else:\n#         with open(cache_path, mode=\"rb\") as f:\n#             dataset_dicts = pickle.load(f)\n    return dataset_dicts\n\ndef chest_dicts_test(images, img_dir = '../input/vinbigdata-competition-jpg-data-3x-downsampled/train/train', load_cache=True):\n#     cache_path = Path(\".\") / f\"dataset_dicts_cache_test.pkl\"\n#     if load_cache == False:\n    dataset_dicts = []\n    for idx, v in enumerate(images):\n        record = {}\n\n        filename = os.path.join(img_dir, v + '.jpg')\n\n        image = cv2.imread(filename)\n        height, width, ch = image.shape\n\n        record[\"file_name\"] = filename\n        record[\"image_id\"] = idx + 12000\n        record[\"height\"] = height # RANDOM Not Req\n        record[\"width\"] = width # RANDOM Not Req\n\n        annos = df[df.image_id == v]\n        objs = []\n        for _, anno in annos.iterrows():\n            if anno.class_id != 14:\n\n                obj = {\n                    \"bbox\": [int(anno.x_min), int(anno.y_min), int(anno.w), int(anno.h)],\n                    \"bbox_mode\": BoxMode.XYWH_ABS,\n                    \"category_id\": int(anno.class_id)\n                }\n                objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n#         with open(cache_path, mode=\"wb\") as f:\n#                 pickle.dump(dataset_dicts, f)\n#     else:\n#         with open(cache_path, mode=\"rb\") as f:\n#             dataset_dicts = pickle.load(f)\n    return dataset_dicts\ndef train():\n    return chest_dicts_train(X_train, load_cache=True) # load_cache=False first time, quicker\ndef val():\n    return chest_dicts_test(X_test, load_cache=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DatasetCatalog - global dict that stores info about dataset. Register once\n\n# DatasetCatalog.register(\"chest_Train\", train) # dataset name ?\n# MetadataCatalog.get(\"chest_Train\").thing_classes=classes\n# DatasetCatalog.register(\"chest_Val\",val)\n# MetadataCatalog.get(\"chest_Val\").thing_classes=classes\n\n\nDatasetCatalog.register(\"chest_Train\", train)\nMetadataCatalog.get(\"chest_Train\").set(thing_classes=classes)\nDatasetCatalog.register(\"chest_Val\",val)\nMetadataCatalog.get(\"chest_Val\").set(thing_classes=classes)\nChest_metadata = MetadataCatalog.get(\"chest_Train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTo calculate & record validation loss\n\nOriginal code from https://medium.com/@apofeniaco/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\nby @apofeniaco\n\"\"\"\nimport numpy as np\nimport logging\n\nfrom detectron2.engine.hooks import HookBase\nfrom detectron2.utils.logger import log_every_n_seconds\nimport detectron2.utils.comm as comm\nimport torch\nimport time\nimport datetime\n\n\nclass LossEvalHook(HookBase):\n    def __init__(self, eval_period, model, data_loader):\n        self._model = model\n        self._period = eval_period\n        self._data_loader = data_loader\n\n    def _do_loss_eval(self):\n        # Copying inference_on_dataset from evaluator.py\n        total = len(self._data_loader)\n        num_warmup = min(5, total - 1)\n\n        start_time = time.perf_counter()\n        total_compute_time = 0\n        losses = []\n        for idx, inputs in enumerate(self._data_loader):\n            if idx == num_warmup:\n                start_time = time.perf_counter()\n                total_compute_time = 0\n            start_compute_time = time.perf_counter()\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            total_compute_time += time.perf_counter() - start_compute_time\n            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n            seconds_per_img = total_compute_time / iters_after_start\n            if idx >= num_warmup * 2 or seconds_per_img > 5:\n                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n                log_every_n_seconds(\n                    logging.INFO,\n                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n                        idx + 1, total, seconds_per_img, str(eta)\n                    ),\n                    n=5,\n                )\n            loss_batch = self._get_loss(inputs)\n            losses.append(loss_batch)\n        mean_loss = np.mean(losses)\n        # self.trainer.storage.put_scalar('validation_loss', mean_loss)\n        comm.synchronize()\n\n        # return losses\n        return mean_loss\n\n    def _get_loss(self, data):\n        # How loss is calculated on train_loop\n        metrics_dict = self._model(data)\n        metrics_dict = {\n            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n            for k, v in metrics_dict.items()\n        }\n        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n        return total_losses_reduced\n\n    def after_step(self):\n        next_iter = int(self.trainer.iter) + 1\n        is_final = next_iter == self.trainer.max_iter\n        if is_final or (self._period > 0 and next_iter % self._period == 0):\n            mean_loss = self._do_loss_eval()\n            self.trainer.storage.put_scalars(validation_loss=mean_loss)\n            print(\"validation do loss eval\", mean_loss)\n        else:\n            pass\n            # self.trainer.storage.put_scalars(timetest=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from detectron2.data import detection_utils as utils\nfrom detectron2.data import build_detection_train_loader\nfrom PIL import Image\n# writing a new DatasetMapper as we have dicom files ?\n\nclass DatasetMapper:\n    \n    \n\n    @configurable\n    def __init__(\n        self,\n        is_train: bool, # whether it’s used in training or inference\n        *,\n        augmentations: List[Union[T.Augmentation, T.Transform]], # a list of augmentations or deterministic transforms to apply\n        image_format: str = 'BGR', # an image format supported by detection_utils.read_image()\n        use_instance_mask: bool = False, # whether to process instance segmentation annotations, if available\n        use_keypoint: bool = False, # whether to process keypoint annotations if available\n        instance_mask_format = \"polygon\", # one of “polygon” or “bitmask”. Process instance segmentation masks into this format\n        keypoint_hflip_indices = None, # see detection_utils.create_keypoint_hflip_indices()\n        precomputed_proposal_topk= None, # if given, will load pre-computed proposals from dataset_dict and keep the top k proposals for each image.\n        recompute_boxes: bool = False, # whether to overwrite bounding box annotations by computing tight bounding boxes from instance mask annotations.\n    ):\n        \"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            is_train: whether it's used in training or inference\n            augmentations: a list of augmentations or deterministic transforms to apply\n            image_format: an image format supported by :func:`detection_utils.read_image`.\n            use_instance_mask: whether to process instance segmentation annotations, if available\n            use_keypoint: whether to process keypoint annotations if available\n            instance_mask_format: one of \"polygon\" or \"bitmask\". Process instance segmentation\n                masks into this format.\n            keypoint_hflip_indices: see :func:`detection_utils.create_keypoint_hflip_indices`\n            precomputed_proposal_topk: if given, will load pre-computed\n                proposals from dataset_dict and keep the top k proposals for each image.\n            recompute_boxes: whether to overwrite bounding box annotations\n                by computing tight bounding boxes from instance mask annotations.\n        \"\"\"\n\n        # fmt: off\n        self.is_train               = is_train\n        self.augmentations          = T.AugmentationList(  augmentations)\n        self.image_format           = image_format\n        \n        # fmt: on\n        logger = logging.getLogger(__name__)\n        mode = \"training\" if is_train else \"inference\"\n        logger.info(f\"[DatasetMapper] Augmentations used in {mode}: {augmentations}\")\n\n    @classmethod\n    def from_config(cls, cfg, is_train: bool = True):\n        augs = utils.build_augmentation(cfg, is_train)\n        if cfg.INPUT.CROP.ENABLED and is_train:\n            augs.insert(0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))\n            recompute_boxes = cfg.MODEL.MASK_ON\n        else:\n            recompute_boxes = False\n\n\n        ret = {\n            \"is_train\": is_train,\n            \"augmentations\": augs,\n            \"image_format\": cfg.INPUT.FORMAT,\n            \"use_instance_mask\": cfg.MODEL.MASK_ON,\n            \"instance_mask_format\": cfg.INPUT.MASK_FORMAT,\n            \"use_keypoint\": cfg.MODEL.KEYPOINT_ON,\n            \"recompute_boxes\": recompute_boxes,\n        }\n\n        return ret\n\n    def __call__(self, dataset_dict):\n        # print(dataset_dict)\n        dataset_dict = copy.deepcopy(dataset_dict)\n        image = utils.read_image(dataset_dict[\"file_name\"])\n        \n        # I'm not sure about these augmentations, need to properly go over them\n        auginput = T.AugInput(image)\n        transform = self.augmentations(auginput)\n        image = np.expand_dims(auginput.image, axis=2).copy()\n        image = torch.from_numpy(image.transpose(2, 0, 1))\n        annos = [\n            utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n            for annotation in dataset_dict.pop(\"annotations\")\n        ]\n        return {\n        # create the format that the model expects\n        \"image\": image,\n        \"instances\": utils.annotations_to_instances(annos, image.shape[1:])\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from detectron2.engine import DefaultTrainer\nfrom detectron2.data import MetadataCatalog, build_detection_train_loader,build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### WE ALSO NEED CUSTOM TRAINER TO TELL DETECTRON TO USE OUR CUSTOM DATASETMAPPER - from RetinaNet\nclass Trainer(DefaultTrainer):\n    \n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        # return COCOEvaluator(dataset_name, cfg, True, output_folder)\n        return COCOEvaluator(dataset_name, tasks=(\"bbox\"), distributed=False, output_dir=output_folder)\n    \n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        return build_detection_test_loader(cfg,dataset_name,\n                mapper=DatasetMapper(cfg, is_train = True)) # was false\n    \n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(dataset =train(), # Get the dataset from train method \n                mapper=DatasetMapper(cfg, is_train = True),\n                aspect_ratio_grouping=False, \n                total_batch_size = cfg.SOLVER.IMS_PER_BATCH)\n    \n#     @classmethod\n#     def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n#         return COCOEvaluator(dataset_name, (\"bbox\",), True, output_dir=output_folder)\n#     could also use VinBigdataEvaluator based on DatasetEvaluator (as in VinBigData detectron2 train)\n# #vinsummarise\n    \n#     def build_hooks(self): # call validation set durting traingin\n#         hooks = super(Trainer, self).build_hooks()\n#         cfg = self.cfg\n#         if len(cfg.DATASETS.TEST) > 0:\n#             loss_eval_hook = LossEvalHook(\n#                 cfg.TEST.EVAL_PERIOD,\n#                 self.model,\n#                 Trainer.build_test_loader(cfg, cfg.DATASETS.TEST[0]),\n#             )\n#             hooks.insert(-1, loss_eval_hook)\n\n#         return hooks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Batch = 10\nEpochs = 1\nsteps = 50  #  ### INCREASE THE STEPS   (len(X_train) // Batch) * Epochs \ncfg = get_cfg()\nNAME = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\" # Chosen model from model zoo for object detection\ncfg.merge_from_file(model_zoo.get_config_file(NAME))\ncfg.DATASETS.TRAIN = (\"chest_Train\",)\ncfg.DATASETS.TEST = (\"chest_Val\", )\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS =  model_zoo.get_checkpoint_url(NAME)\ncfg.SOLVER.IMS_PER_BATCH = Batch\ncfg.CUDNN_BENCHMARK =  True\ncfg.MODEL.RETINANET.NUM_CLASSES  = len(classes)\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\ncfg.SOLVER.MAX_ITER = steps  \ncfg.OUTPUT_DIR = './output'\ncfg.MODEL.PIXEL_MEAN = [103.530]\ncfg.MODEL.PIXEL_STD = [1.0]\ncfg.SOLVER.CHECKPOINT_PERIOD = 1000\ncfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 0.95\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(cfg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.resume_or_load(resume=False)\n# If resume==True and cfg.OUTPUT_DIR contains the last checkpoint (defined by a last_checkpoint file), resume from the file. Resuming means loading all available states (eg. optimizer and scheduler) and update iteration counter from the checkpoint. cfg.MODEL.WEIGHTS will not be used.\n# Otherwise, this is considered as an independent training.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.train() # train + validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test(cfg, NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate insertion over union score","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}